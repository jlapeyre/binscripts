#!/usr/bin/env python3

import os
import re
import json
import argparse
import feedparser

def scirate_url(arxiv_id):
    base_id = re.sub(r'v\d+$', '', arxiv_id)
    return f"https://scirate.com/arxiv/{base_id}"

def extract_arxiv_id_from_filename(filename):
    match = re.search(r'(\d{4}\.\d{4,5}|[a-z\-]+/\d{7})', filename)
    return match.group(1) if match else None

def extract_arxiv_id_from_text(filepath):
    try:
        import fitz  # PyMuPDF
    except ImportError:
        print("PyMuPDF (fitz) not installed, skipping text extraction.")
        return None

    try:
        doc = fitz.open(filepath)
        text = doc[0].get_text()  # Just the first page
        match = re.search(r'arXiv:\s*(\d{4}\.\d{4,5}|[a-z\-]+/\d{7})', text, re.IGNORECASE)
        return match.group(1) if match else None
    except Exception as e:
        print(f"Failed to extract text from {filepath}: {e}")
        return None

def fetch_metadata(arxiv_id):
    url = f'http://export.arxiv.org/api/query?id_list={arxiv_id}'
    feed = feedparser.parse(url)
    if feed.entries:
        entry = feed.entries[0]
        title = entry.title.strip().replace('\n', ' ')
        authors = ', '.join(author.name for author in entry.authors)
        published = entry.published.split('T')[0]
        link = entry.id
        doi = entry.get('arxiv_doi')  # This is where the DOI comes from

        metadata = {
            'title': title,
            'authors': authors,
            'published': published,
            'link': link,
        }
        if doi:
            metadata['doi'] = doi

        return metadata
    return None

# def fetch_metadata(arxiv_id):
#     url = f'http://export.arxiv.org/api/query?id_list={arxiv_id}'
#     feed = feedparser.parse(url)
#     if feed.entries:
#         entry = feed.entries[0]
#         title = entry.title.strip().replace('\n', ' ')
#         authors = ', '.join(author.name for author in entry.authors)
#         published = entry.published.split('T')[0]
#         link = entry.id
#         return {
#             'title': title,
#             'authors': authors,
#             'published': published,
#             'link': link,
#         }
#     return None

def load_cache(path):
    if os.path.exists(path):
        with open(path, 'r') as f:
            return json.load(f)
    return {}

def save_cache(cache, path):
    with open(path, 'w') as f:
        json.dump(cache, f, indent=2)

def main(pdf_dir):
    contents_path = os.path.join(pdf_dir, '00CONTENTS.txt')
    org_path = os.path.join(pdf_dir, 'arxiv.org')
    cache_path = os.path.join(pdf_dir, 'arxiv_metadata.json')

    cache = load_cache(cache_path)
    updated = False
    entries = {}

    for filename in sorted(os.listdir(pdf_dir)):
        if not filename.endswith('.pdf'):
            continue

        filepath = os.path.join(pdf_dir, filename)

        arxiv_id = extract_arxiv_id_from_filename(filename)
        if not arxiv_id:
            arxiv_id = extract_arxiv_id_from_text(filepath)

        if not arxiv_id:
            print(f"No arXiv ID found for: {filename}")
            continue

        if arxiv_id in cache:
            meta = cache[arxiv_id]
        else:
            print(f"Fetching metadata for {arxiv_id}...")
            meta = fetch_metadata(arxiv_id)
            if not meta:
                print(f"Metadata not found for {arxiv_id}")
                continue
            cache[arxiv_id] = meta
            updated = True

        entries[filename] = (arxiv_id, meta)

    if updated:
        save_cache(cache, cache_path)

    with open(contents_path, 'w') as txt_out, open(org_path, 'w') as org_out:
        org_out.write("#+TITLE: arXiv Papers\n\n")

        for filename, (arxiv_id, meta) in sorted(entries.items()):
            title = meta['title']
            authors = meta['authors']
            published = meta['published']
            link = meta['link']
            doi = meta.get('doi')  # May be None

            txt_out.write(f"{arxiv_id} - {title}\n")
            txt_out.write(f"Authors: {authors}\n")
            txt_out.write(f"Date: {published}\n")
            txt_out.write(f"Link: {link}\n\n")
            if doi:
                txt_out.write(f"DOI: https://doi.org/{doi}\n")

            org_out.write(f"* [[shell:zathura {filename} &][{arxiv_id} - {title}]]\n")
            org_out.write(f":PROPERTIES:\n")
            org_out.write(f":Authors: {authors}\n")
            org_out.write(f":Date: {published}\n")
            org_out.write(f":arXiv: {link}\n")
            org_out.write(f":SciRate: {scirate_url(arxiv_id)}\n")
            org_out.write(f":END:\n\n")

if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Generate metadata files for arXiv PDFs with caching and fallback text extraction.')
    parser.add_argument('pdf_dir', help='Path to directory containing arXiv PDFs')
    args = parser.parse_args()
    main(args.pdf_dir)
