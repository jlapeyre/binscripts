#!/usr/bin/env python3

import os
import json
import feedparser
import re
import shutil
import time
import urllib.error

from pathlib import Path

# --- Configuration ---
bibfile = Path("00CONTENTS.txt")
outdir = Path(".")
cache_path = outdir / "arxiv_metadata.json"

# --- Functions ---
def extract_arxiv_id(filename):
    match = re.search(r"(\d{4}\.\d{5})(v\d+)?", filename)
    if match:
        return match.group(1)
    return None

def load_cache(path):
    if path.exists():
        with open(path, 'r') as f:
            return json.load(f)
    return {}

def save_cache(cache, path):
    with open(path, 'w') as f:
        json.dump(cache, f, indent=2)

def fetch_metadata(arxiv_id, retries=3, delay=5):
    url = f'http://export.arxiv.org/api/query?id_list={arxiv_id}'
    for attempt in range(retries):
        try:
            feed = feedparser.parse(url)
            if feed.entries:
                entry = feed.entries[0]
                title = entry.title.strip().replace('\n', ' ')
                authors = ', '.join(author.name for author in entry.authors)
                published = entry.published.split('T')[0]
                link = entry.id
                doi = entry.get('arxiv_doi')

                metadata = {
                    'title': title,
                    'authors': authors,
                    'published': published,
                    'link': link,
                }
                if doi:
                    metadata['doi'] = doi
                return metadata
            return None
        except (ConnectionResetError, urllib.error.URLError) as e:
            print(f"Error fetching {arxiv_id} (attempt {attempt+1}/{retries}): {e}")
            if attempt < retries - 1:
                time.sleep(delay)
            else:
                return None

# --- Load existing metadata cache ---
cache = load_cache(cache_path)

# --- Back up old metadata cache ---
if cache_path.exists():
    shutil.copy2(cache_path, cache_path.with_suffix('.json.bak'))

# --- Scan for PDFs and extract metadata ---
entries = {}
for pdf_path in outdir.glob("*.pdf"):
    arxiv_id = extract_arxiv_id(pdf_path.name)
    if not arxiv_id:
        continue

    if arxiv_id not in cache:
        print(f"Fetching metadata for {arxiv_id} ...")
        meta = fetch_metadata(arxiv_id)
        if meta:
            cache[arxiv_id] = meta
            save_cache(cache, cache_path)  # Save after each fetch
        else:
            print(f"Warning: Failed to fetch metadata for {arxiv_id}")
            continue
    else:
        print(f"Using cached metadata for {arxiv_id}")

    meta = cache[arxiv_id]
    entries[arxiv_id] = {
        "pdf": pdf_path.name,
        **meta
    }

# --- Generate Org-mode file ---
lines = []
for arxiv_id, meta in entries.items():
    entry = f"""* {meta['title']}
:PROPERTIES:
:ArXiv:   {arxiv_id}
:Authors: {meta['authors']}
:PDF:     [[file:{meta['pdf']}]]
:Link:    {meta['link']}"""
    if 'doi' in meta:
        entry += f"\n:DOI:     {meta['doi']}"
    entry += f"\n:Published: {meta['published']}\n:END:\n"
    lines.append(entry)

with open(outdir / "papers.org", "w") as f:
    f.write('\n'.join(lines))

print("âœ“ Done.")


# #!/usr/bin/env python3

# import os
# import re
# import json
# import argparse
# import feedparser

# def scirate_url(arxiv_id):
#     base_id = re.sub(r'v\d+$', '', arxiv_id)
#     return f"https://scirate.com/arxiv/{base_id}"

# def extract_arxiv_id_from_filename(filename):
#     match = re.search(r'(\d{4}\.\d{4,5}|[a-z\-]+/\d{7})', filename)
#     return match.group(1) if match else None

# def extract_arxiv_id_from_text(filepath):
#     try:
#         import fitz  # PyMuPDF
#     except ImportError:
#         print("PyMuPDF (fitz) not installed, skipping text extraction.")
#         return None

#     try:
#         doc = fitz.open(filepath)
#         text = doc[0].get_text()  # Just the first page
#         match = re.search(r'arXiv:\s*(\d{4}\.\d{4,5}|[a-z\-]+/\d{7})', text, re.IGNORECASE)
#         return match.group(1) if match else None
#     except Exception as e:
#         print(f"Failed to extract text from {filepath}: {e}")
#         return None

# def fetch_metadata(arxiv_id):
#     url = f'http://export.arxiv.org/api/query?id_list={arxiv_id}'
#     feed = feedparser.parse(url)
#     if feed.entries:
#         entry = feed.entries[0]
#         title = entry.title.strip().replace('\n', ' ')
#         authors = ', '.join(author.name for author in entry.authors)
#         published = entry.published.split('T')[0]
#         link = entry.id
#         doi = entry.get('arxiv_doi')  # This is where the DOI comes from

#         metadata = {
#             'title': title,
#             'authors': authors,
#             'published': published,
#             'link': link,
#         }
#         if doi:
#             metadata['doi'] = doi

#         return metadata
#     return None

# # def fetch_metadata(arxiv_id):
# #     url = f'http://export.arxiv.org/api/query?id_list={arxiv_id}'
# #     feed = feedparser.parse(url)
# #     if feed.entries:
# #         entry = feed.entries[0]
# #         title = entry.title.strip().replace('\n', ' ')
# #         authors = ', '.join(author.name for author in entry.authors)
# #         published = entry.published.split('T')[0]
# #         link = entry.id
# #         return {
# #             'title': title,
# #             'authors': authors,
# #             'published': published,
# #             'link': link,
# #         }
# #     return None

# def load_cache(path):
#     if os.path.exists(path):
#         with open(path, 'r') as f:
#             return json.load(f)
#     return {}

# def save_cache(cache, path):
#     with open(path, 'w') as f:
#         json.dump(cache, f, indent=2)

# def main(pdf_dir):
#     contents_path = os.path.join(pdf_dir, '00CONTENTS.txt')
#     org_path = os.path.join(pdf_dir, 'arxiv.org')
#     cache_path = os.path.join(pdf_dir, 'arxiv_metadata.json')

#     cache = load_cache(cache_path)
#     updated = False
#     entries = {}

#     for filename in sorted(os.listdir(pdf_dir)):
#         if not filename.endswith('.pdf'):
#             continue

#         filepath = os.path.join(pdf_dir, filename)

#         arxiv_id = extract_arxiv_id_from_filename(filename)
#         if not arxiv_id:
#             arxiv_id = extract_arxiv_id_from_text(filepath)

#         if not arxiv_id:
#             print(f"No arXiv ID found for: {filename}")
#             continue

#         if arxiv_id in cache:
#             meta = cache[arxiv_id]
#         else:
#             print(f"Fetching metadata for {arxiv_id}...")
#             meta = fetch_metadata(arxiv_id)
#             if not meta:
#                 print(f"Metadata not found for {arxiv_id}")
#                 continue
#             cache[arxiv_id] = meta
#             updated = True

#         entries[filename] = (arxiv_id, meta)

#     if updated:
#         save_cache(cache, cache_path)

#     with open(contents_path, 'w') as txt_out, open(org_path, 'w') as org_out:
#         org_out.write("#+TITLE: arXiv Papers\n\n")

#         for filename, (arxiv_id, meta) in sorted(entries.items()):
#             title = meta['title']
#             authors = meta['authors']
#             published = meta['published']
#             link = meta['link']
#             doi = meta.get('doi')  # May be None

#             txt_out.write(f"{arxiv_id} - {title}\n")
#             txt_out.write(f"Authors: {authors}\n")
#             txt_out.write(f"Date: {published}\n")
#             txt_out.write(f"Link: {link}\n\n")
#             if doi:
#                 txt_out.write(f"DOI: https://doi.org/{doi}\n")

#             org_out.write(f"* [[shell:zathura {filename} &][{arxiv_id} - {title}]]\n")
#             org_out.write(f":PROPERTIES:\n")
#             org_out.write(f":Authors: {authors}\n")
#             org_out.write(f":Date: {published}\n")
#             org_out.write(f":arXiv: {link}\n")
#             org_out.write(f":SciRate: {scirate_url(arxiv_id)}\n")
#             org_out.write(f":END:\n\n")

# if __name__ == '__main__':
#     parser = argparse.ArgumentParser(description='Generate metadata files for arXiv PDFs with caching and fallback text extraction.')
#     parser.add_argument('pdf_dir', help='Path to directory containing arXiv PDFs')
#     args = parser.parse_args()
#     main(args.pdf_dir)
