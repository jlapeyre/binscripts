#!/usr/bin/env python3

import shutil
import os
import re
import json
import argparse
import feedparser
import unicodedata
from pathlib import Path
import time
import urllib.error

"""
========================================
arxivorg - A script for managing arXiv papers
========================================

This script processes a directory named "papers," containing PDFs from the arXiv, and generates an Emacs Org mode file for browsing paper titles. The file includes links to local copies, original arXiv entries, and SciRate. Symlinks for the local copies use paper titles in their names, making navigation intuitive without relying on IDs.

Additional Details:
1. Scans the "papers" directory for PDF files containing arXiv IDs.
2. Extracts arXiv IDs from file names or the first page of the PDFs.
3. Fetches metadata from the arXiv API, including titles, authors, publication dates, and links.
4. Creates symlinks in a specified directory with paper titles and arXiv IDs for easier navigation.
5. Caches metadata to avoid repeated API calls and stores it in a JSON file.

Usage:
Run the script from the command line, providing optional parameters for the target directory, link directory, refresh mode, and verbosity.
"""

paper_dir = Path("papers")
bibfile = Path("00CONTENTS.txt")

def scirate_url(arxiv_id):
    base_id = re.sub(r'v\d+$', '', arxiv_id)
    return f"https://scirate.com/arxiv/{base_id}"

def extract_arxiv_id_from_file_name(file_name):
    # Matches modern (2103.12345) or old (hep-th/9901001) style IDs
    match = re.search(r'(\d{4}\.\d{4,5}|[a-z\-]+/\d{7})', file_name)
    return match.group(1) if match else None

def extract_arxiv_id_from_text(filepath):
    try:
        import fitz  # PyMuPDF
    except ImportError:
        print("PyMuPDF (fitz) not installed, skipping text extraction.")
        return None

    try:
        doc = fitz.open(filepath)
        text = doc[0].get_text()  # Just the first page
        match = re.search(r'arXiv:\s*(\d{4}\.\d{4,5}|[a-z\-]+/\d{7})', text, re.IGNORECASE)
        return match.group(1) if match else None
    except Exception as e:
        print(f"Failed to extract text from {filepath}: {e}")
        return None

def clean_file_name(text):
    # Normalize, remove bad chars, replace spaces with underscores
    text = unicodedata.normalize("NFKD", text)
    text = text.replace('/', '_').replace('\\', '_').replace(':', '_')
    # keep letters, numbers, space, dash, underscore, parentheses, dot, comma
    text = re.sub(r'[^\w\s\-\(\)\[\]\.,]', '', text)
    text = re.sub(r'\s+', '_', text).strip('_')
    return text[:100]

def clean_arxiv_id(arxiv_id):
    arxiv_id = re.sub(r'\/', '_', arxiv_id)
    return arxiv_id

def symlink_name(meta):
    # Format: Title_arXivID.pdf
    safe_title = clean_file_name(meta['title'])
    arxiv_id = meta.get('arxiv_id', 'unknown')
    arxiv_id = clean_arxiv_id(arxiv_id)
    return f"{safe_title}_{arxiv_id}.pdf"

def relative_symlink(target_path, link_path):
    # Create relative symlink from link_path pointing to target_path
    rel_target = os.path.relpath(target_path, os.path.dirname(link_path))
    os.symlink(rel_target, link_path)

def fetch_metadata(arxiv_id, retries=3, delay=5):
    url = f'http://export.arxiv.org/api/query?id_list={arxiv_id}'
    for attempt in range(retries):
        try:
            feed = feedparser.parse(url)
            if feed.entries:
                entry = feed.entries[0]
                title = entry.title.strip().replace('\n', ' ')
                authors = ', '.join(author.name for author in entry.authors)
                published = entry.published.split('T')[0]
                link = entry.id
                doi = entry.get('arxiv_doi')

                metadata = {
                    'title': title,
                    'authors': authors,
                    'published': published,
                    'link': link,
                    'arxiv_id': arxiv_id,
                }
                if doi:
                    metadata['doi'] = doi
                return metadata
            return None
        except (ConnectionResetError, urllib.error.URLError) as e:
            print(f"Error fetching {arxiv_id} (attempt {attempt+1}/{retries}): {e}")
            if attempt < retries - 1:
                time.sleep(delay)
            else:
                return None

def load_cache(path):
    if os.path.exists(path):
        with open(path, 'r') as f:
            return json.load(f)
    return {}

def save_cache(cache, path):
    with open(path, 'w') as f:
        json.dump(cache, f, indent=2)

def create_symlink(meta, link_dir, skipped_files):
    link_name = symlink_name(meta)
    link_path = os.path.join(link_dir, link_name)
    file_name = meta['file_name']
    file_path = meta['file_path']
    if os.path.exists(link_path):
        # Avoid overwriting existing symlink or file
        if os.path.islink(link_path):
            existing_target = os.readlink(link_path)
            expected_target = os.path.relpath(file_path, link_dir)
            if existing_target == expected_target:
                print(f"‚úÖ Symlink already exists and correct: {link_name}")
                return link_path
            else:
                print(f"‚ö†Ô∏è Symlink {link_name} points to {existing_target}, expected {expected_target}. Recreating.")
                os.unlink(link_path)
        else:
            print(f"‚ö†Ô∏è {link_name} exists and is not a symlink, skipping to avoid overwrite.")
            skipped_files.append(file_name)
            return link_path
    try:
        relative_symlink(file_path, link_path)
        print(f"üîó Created symlink: {link_name} -> {file_path}")
    except Exception as e:
        print(f"‚ùå Failed to create symlink: {link_name} -> {file_path}")
        print(f"‚ùå Failed to create symlink for {file_name}: {e}")
        failed_files.append(file_name)
    return link_path

def main(target_dir=None, link_dir=None, refresh=False, verbose=False):

    def vprint(*args):
        if verbose:
            print(*args)

    if target_dir is None:
        target_dir = "."
    if link_dir is None:
        link_dir = os.path.join(target_dir, 'arxiv_links')

    paper_path = Path(target_dir) / paper_dir
    # Check for the existence of the "papers" directory
    if not paper_path.is_dir():
        print(f"‚ùå Error: The required directory '{paper_path}' does not exist.")
        return  # Gracefully exit if the directory is missing

    if refresh and os.path.exists(link_dir):
        print(f"Refreshing symlink directory: deleting {link_dir}")
        shutil.rmtree(link_dir)

    os.makedirs(link_dir, exist_ok=True)

    contents_path = os.path.join(target_dir, bibfile)
    org_path = os.path.join(target_dir, 'arxiv.org')
    cache_path = Path(target_dir) / "arxiv_metadata.json"

    if os.path.exists(cache_path):
        vprint(f"Backing up {cache_path}")
        shutil.copy2(cache_path, cache_path.with_suffix('.json.bak'))

    cache = load_cache(cache_path)
    entries = {}
    skipped_files = []
    failed_files = []

    for file_name in sorted(os.listdir(paper_path)):
        if not file_name.endswith('.pdf'):
            continue

        file_path = os.path.join(paper_path, file_name)

        arxiv_id = extract_arxiv_id_from_file_name(file_name)
        if not arxiv_id:
            arxiv_id = extract_arxiv_id_from_text(file_path)

        if not arxiv_id:
            print(f"No arXiv ID found for: {file_name}")
            continue

        if arxiv_id in cache:
            meta = cache[arxiv_id]
        else:
            print(f"Fetching metadata for {arxiv_id}...")
            meta = fetch_metadata(arxiv_id)
            if not meta:
                print(f"Metadata not found for {arxiv_id}")
                continue
            cache[arxiv_id] = meta
            save_cache(cache, cache_path)  # <-- Save immediately

        meta['file_name'] = file_name
        meta['file_path'] = file_path

        # Compose symlink name
        link_name = symlink_name(meta)
        vprint(f"link_name {link_name}")
        link_path = os.path.join(link_dir, link_name)
        vprint(f"link_path {link_path}")

        entries[file_name] = (arxiv_id, meta)

        # Wow this is really ugly
        link_path = create_symlink(meta, link_dir, skipped_files)
        meta['link_path'] = link_path
        vprint(f"link_path {link_path}")
        # dict key link_path is not written on the last entry unless we do this
        save_cache(cache, cache_path)
    with open(contents_path, 'w') as txt_out, open(org_path, 'w') as org_out:
        org_out.write("#+TITLE: arXiv Papers\n\n")

        max_i = len(entries.items())
        for i, (file_name, (arxiv_id, meta)) in enumerate(sorted(entries.items())):
            title = meta['title']
            authors = meta['authors']
            published = meta['published']
            link = meta['link'] # remote link
            link_path = meta['link_path'] # local link
            doi = meta.get('doi')  # May be None

            txt_out.write(f"{arxiv_id} - {title}\n")
            txt_out.write(f"Authors: {authors}\n")
            txt_out.write(f"Date: {published}\n")
            txt_out.write(f"Link: {link}\n\n")

            # this relies on a hook:
            # (org-link-set-parameters
            #  "zathura"
            #  :follow (lambda (path) (start-process "zathura" nil "zathura" path))
            #  :complete (lambda () (read-file-name "Zathura file: "))
            #  :face 'org-link)
            org_out.write(f"* [[zathura:{link_path}][{arxiv_id} - {title}]]\n")
#            org_out.write(f"* [[shell:zathura {link_path} &][{arxiv_id} - {title}]]\n")
            org_out.write(f":PROPERTIES:\n")
            org_out.write(f":Authors: {authors}\n")
            org_out.write(f":Date: {published}\n")
            org_out.write(f":arXiv: {link}\n")
            org_out.write(f":SciRate: {scirate_url(arxiv_id)}\n")
            if doi:
                org_out.write(f":DOI: https://doi.org/{doi}\n")
            org_out.write(f":END:\n")
            if i < max_i - 1:
                org_out.write("\n")

#    print("\nSummary:")
    if len(skipped_files) > 0:
        print(f"Skipped files (no ID): {len(skipped_files)}")
        for f in skipped_files:
            print(f" - {f}")
    if len(failed_files) > 0:
        print(f"Failed symlink creations: {len(failed_files)}")
        for f in failed_files:
            print(f" - {f}")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Generate metadata files for arXiv PDFs with caching and fallback text extraction.')
    parser.add_argument('--target_dir', help='Path to directory with org files and metadata (defaults: "."')
    parser.add_argument('--link-dir', help="Output symlink directory (default: target_dir/arxiv_links)")
    parser.add_argument('-r', '--refresh', action='store_true', help="Clear and rebuild the symlink directory.")
    parser.add_argument('-v', '--verbose', action='store_true', help="Be louder while running.")
    args = parser.parse_args()
    main(args.target_dir, args.link_dir, args.refresh, args.verbose)
    print("‚úì Done.")
