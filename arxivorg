#!/usr/bin/env python3

import shutil
import os
import re
import json
import argparse
import feedparser
import unicodedata
from pathlib import Path
import time
import urllib.error

bibfile = Path("00CONTENTS.txt")
outdir = Path(".")
cache_path = outdir / "arxiv_metadata.json"

def scirate_url(arxiv_id):
    base_id = re.sub(r'v\d+$', '', arxiv_id)
    return f"https://scirate.com/arxiv/{base_id}"

def extract_arxiv_id_from_filename(filename):
    # Matches modern (2103.12345) or old (hep-th/9901001) style IDs
    match = re.search(r'(\d{4}\.\d{4,5}|[a-z\-]+/\d{7})', filename)
    return match.group(1) if match else None

def extract_arxiv_id_from_text(filepath):
    try:
        import fitz  # PyMuPDF
    except ImportError:
        print("PyMuPDF (fitz) not installed, skipping text extraction.")
        return None

    try:
        doc = fitz.open(filepath)
        text = doc[0].get_text()  # Just the first page
        match = re.search(r'arXiv:\s*(\d{4}\.\d{4,5}|[a-z\-]+/\d{7})', text, re.IGNORECASE)
        return match.group(1) if match else None
    except Exception as e:
        print(f"Failed to extract text from {filepath}: {e}")
        return None

def clean_filename(text):
    # Normalize, remove bad chars, replace spaces with underscores
    text = unicodedata.normalize("NFKD", text)
    # keep letters, numbers, space, dash, underscore, parentheses, dot, comma
    text = re.sub(r'[^\w\s\-\(\)\[\]\.,]', '', text)
    text = re.sub(r'\s+', '_', text).strip('_')
    return text[:100]  # limit length

def symlink_name(meta):
    # Format: Title_arXivID.pdf
    safe_title = clean_filename(meta['title'])
    arxiv_id = meta.get('arxiv_id', 'unknown')
    return f"{safe_title}_{arxiv_id}.pdf"

def relative_symlink(target_path, link_path):
    # Create relative symlink from link_path pointing to target_path
    rel_target = os.path.relpath(target_path, os.path.dirname(link_path))
    os.symlink(rel_target, link_path)

def fetch_metadata(arxiv_id, retries=3, delay=5):
    url = f'http://export.arxiv.org/api/query?id_list={arxiv_id}'
    for attempt in range(retries):
        try:
            feed = feedparser.parse(url)
            if feed.entries:
                entry = feed.entries[0]
                title = entry.title.strip().replace('\n', ' ')
                authors = ', '.join(author.name for author in entry.authors)
                published = entry.published.split('T')[0]
                link = entry.id
                doi = entry.get('arxiv_doi')

                metadata = {
                    'title': title,
                    'authors': authors,
                    'published': published,
                    'link': link,
                }
                if doi:
                    metadata['doi'] = doi
                return metadata
            return None
        except (ConnectionResetError, urllib.error.URLError) as e:
            print(f"Error fetching {arxiv_id} (attempt {attempt+1}/{retries}): {e}")
            if attempt < retries - 1:
                time.sleep(delay)
            else:
                return None

# def fetch_metadata(arxiv_id):
#     url = f'http://export.arxiv.org/api/query?id_list={arxiv_id}'
#     feed = feedparser.parse(url)
#     if feed.entries:
#         entry = feed.entries[0]
#         title = entry.title.strip().replace('\n', ' ')
#         authors = ', '.join(author.name for author in entry.authors)
#         published = entry.published.split('T')[0]
#         link = entry.id
#         doi = entry.get('arxiv_doi')  # This is where the DOI comes from

#         metadata = {
#             'title': title,
#             'authors': authors,
#             'published': published,
#             'link': link,
#         }
#         if doi:
#             metadata['doi'] = doi
#         return metadata
#     return None

# def fetch_metadata(arxiv_id):
#     url = f'http://export.arxiv.org/api/query?id_list={arxiv_id}'
#     feed = feedparser.parse(url)
#     if feed.entries:
#         entry = feed.entries[0]
#         title = entry.title.strip().replace('\n', ' ')
#         authors = ', '.join(author.name for author in entry.authors)
#         published = entry.published.split('T')[0]
#         link = entry.id
#         return {
#             'title': title,
#             'authors': authors,
#             'published': published,
#             'link': link,
#         }
#     return None

def load_cache(path):
    if os.path.exists(path):
        with open(path, 'r') as f:
            return json.load(f)
    return {}

def save_cache(cache, path):
    with open(path, 'w') as f:
        json.dump(cache, f, indent=2)

def main(pdf_dir=None, link_dir=None, refresh=False):
    if pdf_dir is None:
        pdf_dir = "."
    if link_dir is None:
        link_dir = os.path.join(pdf_dir, 'arxiv_links')

    if refresh and os.path.exists(link_dir):
        print(f"Refreshing symlink directory: deleting {link_dir}")
        shutil.rmtree(link_dir)

    os.makedirs(link_dir, exist_ok=True)


    contents_path = os.path.join(pdf_dir, bibfile)
    org_path = os.path.join(pdf_dir, 'arxiv.org')
    cache_path = Path(pdf_dir) / "arxiv_metadata.json"
#    cache_path = os.path.join(pdf_dir, cache_path)

#    backup_path = cache_path + '.bak'
    if os.path.exists(cache_path):
        shutil.copy2(cache_path, cache_path.with_suffix('.json.bak'))

    cache = load_cache(cache_path)
    updated = False
    entries = {}
    skipped_files = []
    failed_files = []

    for filename in sorted(os.listdir(pdf_dir)):
        if not filename.endswith('.pdf'):
            continue

        filepath = os.path.join(pdf_dir, filename)

        arxiv_id = extract_arxiv_id_from_filename(filename)
        if not arxiv_id:
            arxiv_id = extract_arxiv_id_from_text(filepath)

        if not arxiv_id:
            print(f"No arXiv ID found for: {filename}")
            continue

        if arxiv_id in cache:
            meta = cache[arxiv_id]
        else:
            print(f"Fetching metadata for {arxiv_id}...")
            meta = fetch_metadata(arxiv_id)
            if not meta:
                print(f"Metadata not found for {arxiv_id}")
                continue
            cache[arxiv_id] = meta
            save_cache(cache, cache_path)  # <-- Save immediately

            updated = True



        # Compose symlink name
        link_name = symlink_name(meta)
        link_path = os.path.join(link_dir, link_name)
        meta['link_path'] = link_path
        entries[filename] = (arxiv_id, meta)

        if os.path.exists(link_path):
            # Avoid overwriting existing symlink or file
            if os.path.islink(link_path):
                existing_target = os.readlink(link_path)
                expected_target = os.path.relpath(filepath, link_dir)
                if existing_target == expected_target:
                    print(f"✅ Symlink already exists and correct: {link_name}")
                    continue
                else:
                    print(f"⚠️ Symlink {link_name} points to {existing_target}, expected {expected_target}. Recreating.")
                    os.unlink(link_path)
            else:
                print(f"⚠️ {link_name} exists and is not a symlink, skipping to avoid overwrite.")
                skipped_files.append(filename)
                continue

        try:
            relative_symlink(filepath, link_path)
            print(f"🔗 Created symlink: {link_name} -> {filepath}")
        except Exception as e:
            print(f"❌ Failed to create symlink for {filename}: {e}")
            failed_files.append(filename)


    # Not needed because we are saving on every new fetch
    # if updated:
    #     save_cache(cache, cache_path)

    with open(contents_path, 'w') as txt_out, open(org_path, 'w') as org_out:
        org_out.write("#+TITLE: arXiv Papers\n\n")

        for filename, (arxiv_id, meta) in sorted(entries.items()):
            title = meta['title']
            authors = meta['authors']
            published = meta['published']
            link = meta['link'] # remote link
            link_path = meta['link_path'] # local link
            doi = meta.get('doi')  # May be None

            txt_out.write(f"{arxiv_id} - {title}\n")
            txt_out.write(f"Authors: {authors}\n")
            txt_out.write(f"Date: {published}\n")
            txt_out.write(f"Link: {link}\n\n")

            # this relies on a hook:
            # (org-link-set-parameters
            #  "zathura"
            #  :follow (lambda (path) (start-process "zathura" nil "zathura" path))
            #  :complete (lambda () (read-file-name "Zathura file: "))
            #  :face 'org-link)
            org_out.write(f"* [[zathura:{link_path}][{arxiv_id} - {title}]]\n")
#            org_out.write(f"* [[shell:zathura {link_path} &][{arxiv_id} - {title}]]\n")
            org_out.write(f":PROPERTIES:\n")
            org_out.write(f":Authors: {authors}\n")
            org_out.write(f":Date: {published}\n")
            org_out.write(f":arXiv: {link}\n")
            org_out.write(f":SciRate: {scirate_url(arxiv_id)}\n")
            if doi:
                org_out.write(f":DOI: https://doi.org/{doi}\n")
            org_out.write(f":END:\n\n")

#    print("\nSummary:")
    if len(skipped_files) > 0:
        print(f"Skipped files (no ID): {len(skipped_files)}")
        for f in skipped_files:
            print(f" - {f}")
    if len(failed_files) > 0:
        print(f"Failed symlink creations: {len(failed_files)}")
        for f in failed_files:
            print(f" - {f}")


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='Generate metadata files for arXiv PDFs with caching and fallback text extraction.')
    parser.add_argument('--pdf_dir', help='Path to directory containing arXiv PDFs. (defaults: "."')
    parser.add_argument('--link-dir', help="Output symlink directory (default: pdf_dir/arxiv_links)")
    parser.add_argument('--refresh', action='store_true', help="Clear and rebuild the symlink directory")
    args = parser.parse_args()
    main(args.pdf_dir, args.link_dir, args.refresh)
    print("✓ Done.")
